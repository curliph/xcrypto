% =============================================================================

\section{Introduction}

% =============================================================================

\section{Related work}

% =============================================================================

\section{Design decisions and discussion}

\begin{itemize}

\item The current design is a 
      greenfield, non-standard extension~\cite[Section 21.1]{SCARV:RV:ISA:I:17} 
      to the RISC-V 
      RV32I~\cite[Section 2]{SCARV:RV:ISA:I:17}
      or
      RV32E~\cite[Section 3]{SCARV:RV:ISA:I:17}
      base 
      ISA.

\item The current design uses an instruction encoding which populates the
      so-called $custom-1$ space~\cite[Table 19.1]{SCARV:RV:ISA:I:17}; it
      uses a $25$-bit encoding space with prefix $\RADIX{0101011}{2}$.

\item The current design is intended to be agnostic wrt. the host core it
      is coupled to, and there is, conceptually, no reason they cannot be 
      used within RV64I and RV128I.

      Likewise, if treated as a prototype, many constituent concepts could
      be mapped onto alternative, brownfield extensions to RV32I or RV32E.
      As an example, consider the multi-precision integer operations 
      (per \REFSEC{sec:XXX})
      which {\em could} be adopted in RV32I (having been re-specified to
      operate on the general-purpose register file).

\item The current design assumes use of a memory interface shared with the
      host core, i.e., there is no dedicated co-processor memory interface.
      As such, several points need to be clarified:

      \begin{itemize}
      \item Attack vectors (see, e.g.,~\cite{SCARV:GYCH:18}) that (ab)use 
            the memory interface {\em must} be robustly mitigated, as they
            would also have to be wrt. the host core: the currently design 
            delivers no countermeasures itself in this regard.
      \item Per~\cite{SCARV:GeYarHei:18}, acting on the above through the
            enforcement of time protection demands mechanisms to
            a) strictly partition
               and/or 
            b) cleanly reset
            pertinent (micro-)architectural resources.
            Support for either mechanism seems patchy (at best) in RISC-V,
            and certainly not considered in a first-class manner (bar some
            of the privileged architectures~\cite{SCARV:RV:ISA:II:17} wrt.
            flushing the TLB and so on).  As such, there is potential to 
            do so, either
            a) in a manner realised via the co-processor,
               or
            b) as a related extension.
            Note that related features are starting to appear in concrete 
            host cores
            (e.g., the E31 re-configurable instruction cache~\cite[Section 3.1.1]{SCARV:SiFive:E31}),
            although not {\em necessarily} targeted at this purpose.
      \end{itemize}

\item Although the current design attempts to follow the RISC-V ethos of 
      avoiding unnecessary additional state, it {\em does} introduce some:
      it includes an additional $16$-entry co-processor register file.  

      It is important to recognise 
      the overhead, wrt. both 
      time (e.g., due to it needing to be context switched) 
      and 
      space (e.g., due to the logic required),
      this (or {\em any} additional state) implies.
      However, it seems reasonable to align this overhead with that of any
      extension for floating-point arithmetic (which, in the same way, will
      typically make use of a dedicated floating-point register file: many 
      of the trade-offs involved, e.g., provision of
      a) clear separation of duty,
      b) additional capacity,
         and
      c) additional (specialised) functionality,
      are the same, but motivated by cryptographic workloads.
      Due to the nature of such workloads, however, attack vectors such as
      LazyFP~\cite{SCARV:StePre:18}, which capitalises on a short-cut wrt. 
      the overhead of context switching, {\em must} be robustly mitigated.

\item Lee and Fiskiran introduce (e.g., see~\cite{SCARV:LeeFis:05}) the PLX
      design, which, among other concepts, supports 
      a) sub-word parallelism (cf. packed, or SWAR-like operations),
         and
      b) word size scalability (or data-path scalability).
      First, 
      note that a fully sub-word parallel ISA is orthogonal wrt. all useful
      sub-word sizes; PLX supports $n$-byte sub-word sizes for sane $n$.
      Second,
      note that, in a sense, RISC-V is word size scalable: it can cater for
      implementations that, e.g., have a $32$-bit instructions but a $32$-,
      $64$-, or $128$-bit general-purpose register file (and address space).
      The same concept is useful wrt. the co-processor, and indeed, for the
      multi-precision integer operations (per \REFSEC{sec:XXX})
      in particular, perhaps even more so than normal: such operations will
      naturally benefit from a larger word size.

      The current design considers sub-{\em byte} (e.g., $4$-bit), sub-word
      sizes; doing so is motivated, e.g., by their utility in some classes 
      of light-weight block cipher.  However, it does not currently explore 
      the potential of a scalable word size: it assumes the general-purpose
      and co-processor register file word sizes are {\em both} $32$ bits.

\item Consider a general case, wherein a given ISA has instruction formats
      to allow access to $n$ general-purpose registers, st. 
      $
      n = s + d
      $ 
      for $s$ sources and $d$ destinations, meaning an associated encoding 
      must somehow specify $n$ register addresses.  The special case
      $
      n = 3 = 2 + 1 
      $
      is common, and adopted by RISC-V, but it should nevertheless be clear
      that {\em other} cases can also be useful.  A common ``wrinkle'' in a 
      strict $3$-address case is (full) $( w \times w )$-bit multiplication, 
      which produces a $(2 \cdot w )$-bit product and therefore demands use 
      of {\em two} $w$-bit destination registers.
   
      One of the strategies underlying the co-processor design support for 
      larger $s$ and/or $d$.  This is rationalised by a (ideally positive) 
      trade-off between
      a) increased register file complexity, as a result of the requirement
         to support $n$ ports, or, alternatively, multi-cycle operations,
         vs.
      b) increased register file bandwidth.
      The latter enables each instruction to perform more, or richer forms 
      of computation, aligning well with the demands of many cryptographic 
      workloads.  Even {\em if} said trade-off is acceptable, however, it 
      also implies some challenges wrt. instruction encoding.  There seem
      to be several possible options:

      \begin{enumerate}
      \item One could make all  register addresses {\em explicit}.
            For example, XS1 uses a long (i.e., $32$-bit) $6$-address 
            instruction format~\cite[Page 246]{SCARV:XS1:09} 
            to encode
            {\tt lmul}~\cite[Page 146]{SCARV:XS1:09}.
      \item One could make some register addresses {\em implicit}.  
            For example, the x86 $( 32 \times 32 )$-bit multiplication 
            instruction 
            {\tt mull}~\cite[Page 4-144]{SCARV:X86:2a:12} 
            makes implicit uses of {\tt edx} and {\tt edx} as destinations.
      \item One could make some register addresses {\em implied}.
            For example, this approach has been considered within the
            specific context of support for cryptography: 
            Lee and Choi~\cite{SCARV:LeeCho:08} propose Register File
            Extension for Multi-word and Long-word Operation (RFEMLO), 
            where a group of $n$ contiguous registers is identified by 
            one register address plus a group size (or level in their terminology): 
            address $i$ and level $n$ implies use of registers
            \[
            i, i + 1, i + 2, \ldots i + 2^n - 1 .
            \]
            Note that this approach potentially causes an issue wrt.
            registers with specific semantics.  For example, in many
            RISC-like ISAs (including RISC-V), $\GPR[0]$ is fixed to 
            $0$; it may be difficult to include or exclude $\GPR[0]$ 
            in a group as need be.
      \item One could make some register addresses {\em overloaded}.
            For example, ARMv7-A includes a so-called ``unsigned multiply,
            accumulate accumulate'' instruction 
            {\tt umaal}~\cite[Section A8.8.255]{SCARV:ARMv7_M:17} 
            whose format {\em suggests} $n = 4 = 2 + 2$ but in fact 
            reuses the two destination as additional sources.
      \end{enumerate}
      
      \noindent
      Note that several of these approaches have an implication for the
      difficulty of register allocation; the obvious example is that of
      implicit register addresses.  Likewise, there are various generic
      ways to mitigate the encoding pressure (i.e., the availability of 
      at most $w$ bits) given an approach.  For example one could
      
      \begin{enumerate}
      \item restrict access to some subset of the register file 
            (cf. ARM Thumb or RV32E) 
            thereby reducing the number of bits required to encode each  
            register address,
            or
      \item use some form of instruction prefix.
      \end{enumerate}
      
      \noindent
      The current design is somewhat conservative, in the sense it 
      a) supports (upto) $d = 2$ and $s = 3$, 
         optionally using implied specification option for destinations: 
         one register address is explicit and the other can be implied,
      b) can have short register addresses than normal, due to the use
         of a $16$-entry co-processor register file.
      It does not explore the potential for, and trade-offs wrt. larger 
      $d$ or $s$, which are intuitively useful in specific cases.

\end{itemize}

% =============================================================================

\subsection{Multi-precision (modular) integer arithmetic}

Multi-precision (modular) integer arithmetic represents the foundation
of many cryptographic use-cases, e.g., asymmetric algorithms such as
RSA (relying on arithmetic in $\B{Z}_N$ for large $N$),
and
ECC (relying on arithmetic in $\B{F}_p$ for large $p$).
As such, there is a large body of literature on the implementation of
said arithmetic; this includes {\em support} for implementations via
ISEs defined for RISC-based processors
(see, e.g.,\cite{SCARV:GroKam:03,SCARV:GroKam:04}).

The following ISE mirrors a set of ``long arithmetic'' instructions in
XS1~\cite[Section 18]{SCARV:XS1:09}, which, in turn, mirror the set of
primitive operations typical of multi-precision integer arithmetic.  
A central design principle is that each instruction explicitly accepts 
all operands required: no additional state, e.g., accumulator register
or status flags (e.g., carry or borrow), is required.  Note that such
an approach is particularly attractive within the context of RV32, as
a result of it omitting status flags in favour of software-based carry
and overflow detection.

\subsection{Compressed Boolean operations (via LUTs)}

As introduced by Biham~\cite{SCARV:Biham:97}, bit-slicing is based on
a) a non-standard {\em representation} of data,
   and
b) a non-standard {\em implementation} of functions, which operate on
   said representations:
essentially it describes some cryptographic primitive (e.g., a block
cipher) as a  ``software circuit'' comprising a sequence of bit-wise 
instructions (e.g., NOT, AND, and OR).  Although not general-purpose,
bit-slicing offers advantages including constant-time execution and 
hence immunity from cache-based side-channel attacks.

The proposal of Serpent~\cite[Page 232]{SCARV:BihAndKnu:98} includes 
a suggestion for accelerating bit-sliced implementations through use 
of a ``BITSLICE instruction''; this suggestion was later investigated 
in a more concrete sense by Grabher et al.~\cite{SCARV:GraGroPag:08}.
In both, the idea is to ``compress'' a sub-circuit, i.e., a sequence 
of bit-wise instructions representing an $n$-input Boolean function,
into a Look-Up Table (LUT): essentially, the LUT is first configured 
with a truth table for the function, and then accessed to apply said
function.  

Two options for capitalising on this concept are outlined below:

\begin{itemize}
\item the first option mirrors~\cite{SCARV:GraGroPag:08}:
      it introduces some state (namely two special-purpose registers)
      for the LUTs, and supports the simultaneous application of two 
      $4$-bit functions,
\item the second is stateless (i.e., removes those registers), but at 
      the cost of supporting application of one $3$-input function.
\end{itemize}

% =============================================================================
